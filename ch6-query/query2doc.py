from llama_index.core.indices import GPTListIndex
from llama_index.core import Document, VectorStoreIndex
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS  # pip install faiss-cpu

# Step 1: Initialize the LLM for pseudo-document generation
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# Step 2: Define Documents (Sample Corpus)
documents = [
    Document(text="LangChain is a framework for building applications with LLMs."),
 	Document(text="LangChain Development: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support. "),
    Document(text="LlamaIndex helps with data indexing and retrieval using embeddings."),
    Document(text="Pseudo-documents can improve query understanding in dense retrieval.")
]


# Step 3: Create a Dense Retrieval Index
texts = [doc.text for doc in documents]
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(texts, embeddings)

# Step 4: Generate Pseudo-Documents Using LLM
def generate_pseudo_documents(query):
    """
    Generate pseudo-documents for a given query using an LLM.
    Args:
        query (str): The original user query.
    Returns:
        list: A list of pseudo-documents generated by the LLM.
    """
    prompt = f"Generate a pseudo-document that provides context or potential answers to the query:\n\nQuery: {query}\n\nPseudo-document:"
    pseudo_doc = llm.predict(prompt)
    return [pseudo_doc.strip()]

# Step 5: Expand the Query (Query + Pseudo-Documents)
def expand_query(query, pseudo_docs):
    """
    Expand the query by concatenating the original query with pseudo-documents.
    Args:
        query (str): The original user query.
        pseudo_docs (list): Pseudo-documents generated by the LLM.
    Returns:
        str: The expanded query.
    """
    expanded_query = query + " [SEP] " + " [SEP] ".join(pseudo_docs)
    return expanded_query

# Step 6: Perform Retrieval with Expanded Query
def retrieve_with_expanded_query(query, vectorstore):
    """
    Retrieve documents using the expanded query.
    Args:
        query (str): The expanded query.
        vectorstore: The dense retrieval vectorstore.
    Returns:
        list: Retrieved documents.
    """
    # Convert the expanded query into embeddings and perform similarity search
    results = vectorstore.similarity_search(query, k=3)
    return results

# Step 7: Combine All Steps into Query2doc Workflow
def query2doc_workflow(original_query):
    # Generate pseudo-documents
    pseudo_docs = generate_pseudo_documents(original_query)
    print("Generated Pseudo-Documents:", pseudo_docs)
    
    # Expand the query
    expanded_query = expand_query(original_query, pseudo_docs)
    print("Expanded Query:", expanded_query)
    
    # Retrieve documents using the expanded query
    retrieved_docs = retrieve_with_expanded_query(expanded_query, vectorstore)
    return retrieved_docs

# Step 8: Execute the Query2doc Workflow
user_query = "What is LangChain framework?"
retrieved_docs = query2doc_workflow(user_query)

# Display Results
print("\nRetrieved Documents:")
for doc in retrieved_docs:
    print(doc.page_content)

